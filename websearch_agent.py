from ddgs import DDGS
from groq import Groq
import json
from dotenv import load_dotenv
import os

load_dotenv()

# Initialize Groq client and model same as gs_agent.py
client = Groq()
MODEL = "openai/gpt-oss-120b"  # Use same model as gs_agent

def search_web(query, num_results=20):
    """Search the web using DuckDuckGo API (ddgs)."""
    try:
        results = []
        with DDGS(timeout=20) as ddgs:
            for result in ddgs.text(query, max_results=num_results):
                # print(result)
                results.append({
                    'title': result.get('title', ''),
                    'url': result.get('href', ''),
                    'snippet': result.get('body', '')
                })
        return results
    except Exception as e:
        return [{'error': f"Search failed: {str(e)}"}]

def synthesize_answer_with_llm(user_query, search_results):
    # Format all results for LLM prompt
    context = f"You are an assistant that synthesizes the best answer from multiple web search results.\nUser query: {user_query}\nSearch results:\n"
    for idx, res in enumerate(search_results, 1):
        if 'error' in res:
            context += f"Result {idx}: ERROR: {res['error']}\n"
        else:
            context += (f"\nResult {idx}:\nTitle: {res['title']}\nURL: {res['url']}\nSnippet: {res['snippet']}\n")

    context += "\nProvide a comprehensive, concise answer to the user's question based on the above search results. " \
               "At the end, clearly state: 'This result was generated by an LLM based on web search results.'"

    messages = [
        {"role": "system", "content": "You are a helpful assistant that synthesizes web search results."},
        {"role": "user", "content": context}
    ]

    # Call Groq LLM with formatted prompt
    response = client.chat.completions.create(
        model=MODEL,
        messages=messages,
        max_tokens=800,
        stream=False
    )
    return response.choices[0].message.content


def run_websearch_agent(user_query):
    print(f"Searching the web for: {user_query}")
    search_results = search_web(user_query)
    # Send results to LLM for answer synthesis
    try:
        synthesized_answer = synthesize_answer_with_llm(user_query, search_results)
        return synthesized_answer
    except Exception as exc:
        # Fallback: return raw results if LLM call fails
        fallback_response = "[LLM Synthesis failed, showing raw results]\n"
        for idx, item in enumerate(search_results, 1):
            if 'error' in item:
                fallback_response += f"Result {idx}: {item['error']}\n"
            else:
                fallback_response += (f"\nResult {idx}:\nTitle: {item['title']}\nURL: {item['url']}\nSnippet: {item['snippet']}\n")
        return fallback_response