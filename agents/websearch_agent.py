from ddgs import DDGS
from groq import Groq
import json
from dotenv import load_dotenv
import os
import requests
from bs4 import BeautifulSoup
import time

load_dotenv()

# Initialize Groq client and model same as gs_agent.py
client = Groq()
MODEL = "openai/gpt-oss-120b"  # Use same model as gs_agent

def search_web(query, num_results=3):
    """
    Searches DuckDuckGo and then scrapes the full text from the URLs.
    No API Key required.
    """
    print(f"Searching DDG for: {query}...")
    
    links = []
    
    # --- 1. Get URLs from DuckDuckGo ---
    try:
        with DDGS() as ddgs:
            # ddgs.text() returns an iterator of results
            results = ddgs.text(query, max_results=num_results)
            if results:
                for result in results:
                    links.append(result['href'])
            else:
                print("No results found.")
                return []
    except Exception as e:
        print(f"DDG Search failed: {e}")
        return []

    full_data = []

    # --- 2. Visit each URL and extract text ---
    for url in links:
        try:
            print(f"Scraping: {url}")
            
            # Use a standard browser User-Agent to avoid being blocked
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            # Timeout is important so the script doesn't hang on slow sites
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract text from paragraphs
                paragraphs = [p.get_text().strip() for p in soup.find_all('p')]
                
                # Join paragraphs and clean up whitespace
                full_text = '\n\n'.join([p for p in paragraphs if p])
                
                if full_text:
                    full_data.append({
                        'url': url,
                        'title': soup.title.string if soup.title else "No Title",
                        'content': full_text[:10000] + "..." # Limit text to prevent overload
                    })
            else:
                print(f"Skipped {url} (Status: {response.status_code})")
                
        except Exception as e:
            print(f"Failed to scrape {url}: {e}")

    return full_data

def synthesize_answer_with_llm(user_query, search_results):
    # Format all results for LLM prompt
    context = f"You are an assistant that synthesizes the best answer from multiple web search results.\nUser query: {user_query}\nSearch results:\n"
    for idx, res in enumerate(search_results, 1):
        if 'error' in res:
            context += f"Result {idx}: ERROR: {res['error']}\n"
        else:
            context += (f"\nResult {idx}:\nTitle: {res['title']}\nURL: {res['url']}\nContent: {res['content']}\n")

    context += "\nProvide a comprehensive, concise answer to the user's question based on the above search results. " \
               "At the end, clearly state: 'This result was generated by an LLM based on web search results.'"

    messages = [
        {"role": "system", "content": "You are a helpful assistant that synthesizes web search results."},
        {"role": "user", "content": context}
    ]

    # Call Groq LLM with formatted prompt
    response = client.chat.completions.create(
        model=MODEL,
        messages=messages,
        max_tokens=800,
        stream=False
    )
    return response.choices[0].message.content


def run_websearch_agent(user_query):
    print(f"Searching the web for: {user_query}")
    search_results = search_web(user_query)
    # Send results to LLM for answer synthesis
    try:
        synthesized_answer = synthesize_answer_with_llm(user_query, search_results)
        return synthesized_answer
    except Exception as exc:
        # Fallback: return raw results if LLM call fails
        fallback_response = "[LLM Synthesis failed, showing raw results]\n"
        for idx, item in enumerate(search_results, 1):
            if 'error' in item:
                fallback_response += f"Result {idx}: {item['error']}\n"
            else:
                fallback_response += (f"\nResult {idx}:\nTitle: {item['title']}\nURL: {item['url']}\nContent: {item['content']}\n")
        return fallback_response